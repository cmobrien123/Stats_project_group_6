---
title: "R Notebook"
output: html_notebook
---



# pulling in DFs and Merging (still needed?)
```{r}
# #red wines
# Red_wine <- read.csv("wineQualityReds.csv", header=TRUE, sep = ",")
# Red_wine$Type <- 'Red'
# 
# #white wines
# White_wine <- read.csv("wineQualityWhites.csv", header=TRUE, sep = ",")
# White_wine$Type <- 'White'
# 
# ## consolidated
# Data <- rbind(Red_wine,White_wine)
# drops <- c("X")
# Data <- Data[ , !(names(Data) %in% drops)]
# Data
# 
# ##create final DF
# # write.csv(Data,"/Users/colinobrien/Desktop/repo/stats_6021/Stats_project_group_6/Data.csv", row.names = FALSE)
# # write.csv(Data,"/Users/colinobrien/Desktop/repo/stats_6021/Stats_project_group_6/Data", row.names = FALSE)
# ## both the Data and Data csv are the same. I know people prefer one format vs the other so I made both

```




```{r}
library(tidyverse)
# library(ROCR)
library(faraway)
library(dplyr)
library(ggplot2)
library(reshape2)
library(leaps)
# install.packages("bestglm")
library(bestglm)
# install.packages("performance")
# library(performance)
knitr::opts_chunk$set(echo = TRUE)



## Load Datasets
full_wines_final <- read.csv("Data_Final.csv", header = TRUE, stringsAsFactors=TRUE)
# Drop quality for simplicity
full_wines_binary_with_qual<-full_wines_final
full_wines_binary <- subset(full_wines_final, select = -c(quality))
## Convert to 0 and 1 for readability
full_wines_binary$cat_quality <- as.integer(full_wines_binary$cat_quality == "High")

set.seed(90210) ##for reproducibility
sample<-sample.int(nrow(full_wines_binary), floor(.80*nrow(full_wines_binary)), replace = F)
train<-full_wines_binary[sample, ] ##training data frame
rownames(train) <- c(1:5197)
test<-full_wines_binary[-sample, ] ##test data frame

## Just for a single boxplot
train_with_qual<-full_wines_binary_with_qual[sample,]
test_with_qual<-full_wines_binary_with_qual[-sample,]


train
```
# EDA


```{r}
# drops_cats <- c("Type")
# No_cat_train <- train[ , !(names(train) %in% drops_cats)]
# # No_Type

pairs(train, lower.panel = NULL)
```

```{r}
# Convert Type to binary to 0 and 1 for correlation
train$Type <- as.integer(train$Type == "White")
test$Type <- as.integer(test$Type == "White")
cor_train <- cor(train)
cor_train
```



```{r}
T_F_cor <- abs(cor_train)>.7
T_F_cor
```
```{r}
## create melted
melted_cor_train <- melt(cor_train)

##create heat map Consolidated
ggplot(data = melted_cor_train, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'Consolidated (Both Red and White)')




```

```{r}
## creating red and white
train_White <- filter(train, Type == 1)
train_Red <- filter(train, Type == 0)


## droping red/white columns
train_White_NoType <- subset(train_White, select = -c(Type))
train_Red_NoType <- subset(train_Red, select = -c(Type))

## creating correlations
cor_train_White_NoType <- cor(train_White_NoType)
cor_train_Red_NoType <- cor(train_Red_NoType)

## melting
melted_cor_train_white <- melt(cor_train_White_NoType)
melted_cor_train_Red <- melt(cor_train_Red_NoType)

##ploting

##create heat map White
ggplot(data = melted_cor_train_white, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'White Wine')

##create heat map Red
ggplot(data = melted_cor_train_Red, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'Red Wine')

```

```{r}
ggplot(data = train, mapping = aes(x=Type)) + 
  geom_bar()
```

# Regression Testing

```{r}
## press formula (from class)
get_press <- function(model) {
  sum(((model$residuals)/ (1- (lm.influence(model)$hat)))^2)
}
```


```{r}
## first go
full<-glm(cat_quality~., family=binomial, data=train)
summary(full)
```
```{r}
## removed all insignificant
reduced_1<-glm(formula = cat_quality~volatile.acidity+residual.sugar+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol+Type, family=binomial, data=train)
summary(reduced_1)
```

```{r}
##evaluating model
Reduced1_AIC_train <- reduced_1$aic

##predicted quality for test data based on training data
preds<-predict(reduced_1,newdata=test, type="response")

reduced_1_error <- table(test$cat_quality, preds>0.5)

reduced_1_error

evulation_summary <- data.frame(
  attempt = 'reduced_1',
  AIC = Reduced1_AIC_train,
  PRESS = get_press(reduced_1),
  'False positive' = round(reduced_1_error[3]/(reduced_1_error[1]+reduced_1_error[3]),3),
  'False negative' = round(reduced_1_error[2]/(reduced_1_error[2]+reduced_1_error[4]),3),
  'Error Rate' = round((reduced_1_error[2]+reduced_1_error[3])/(reduced_1_error[1]+reduced_1_error[2]+reduced_1_error[3]+reduced_1_error[4]),3)
)
evulation_summary
```

## second model
## https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html

```{r}
# install.packages("bestglm")
## Prepare data
train.for.best.logistic <- within(train, {
    y <- cat_quality 
})

## Reorder variables
train.for.best.logistic <-
    train.for.best.logistic[, c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","total.sulfur.dioxide","density","chlorides","free.sulfur.dioxide",'pH','sulphates','alcohol','Type',"y")]

## Perform
res.best.logistic <-
    bestglm(Xy = train.for.best.logistic,
            family = binomial,          # binomial family for logistic
            IC = "AIC",                 # Information criteria for
            method = "exhaustive")
```


```{r}
res.best.logistic$BestModels
summary(res.best.logistic$BestModel)
```
```{r}
reduced_4 <- res.best.logistic$BestModel
##evaluating model
Reduced4_AIC_train <- reduced_4$aic

##predicted quality for test data based on training data
preds<-predict(reduced_4,newdata=test, type="response")

reduced_4_error <- table(test$cat_quality, preds>0.5)

evulation_summary_4 <- data.frame(
  attempt = 'reduced_4_error (all possible)',
  AIC = Reduced4_AIC_train,
  PRESS = get_press(reduced_4),
  'False positive' = round(reduced_4_error[3]/(reduced_4_error[1]+reduced_4_error[3]),3),
  'False negative' = round(reduced_4_error[2]/(reduced_4_error[2]+reduced_4_error[4]),3),
  'Error Rate' = round((reduced_4_error[2]+reduced_4_error[3])/(reduced_4_error[1]+reduced_4_error[2]+reduced_4_error[3]+reduced_4_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4)
# evulation_summary
```

```{r}
# data.frame(check_collinearity(reduced_4))










#come back and add df stuff
```

## in an effort to lower VIFs scores and correlation, I am removing fixed.acidity

```{r}
reduced_4_2<-glm(cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type, family=binomial, data=train)
summary(reduced_4_2)
```
```{r}
##evaluating model
Reduced4_2_AIC_train <- reduced_4_2$aic
##predicted quality for test data based on training data
preds<-predict(reduced_4_2,newdata=test, type="response")
reduced_4_2_error <- table(test$cat_quality, preds>0.7)
#Curves
evulation_summary_4_2 <- data.frame(
  attempt = 'reduced_4_2_error (post VIF adjustments)',
  AIC = Reduced4_2_AIC_train,
  PRESS = get_press(reduced_4_2),
  'False positive' = round(reduced_4_2_error[3]/(reduced_4_2_error[1]+reduced_4_2_error[3]),3),
  'False negative' = round(reduced_4_2_error[2]/(reduced_4_2_error[2]+reduced_4_2_error[4]),3),
  'Error Rate' = round((reduced_4_2_error[2]+reduced_4_2_error[3])/(reduced_4_2_error[1]+reduced_4_2_error[2]+reduced_4_2_error[3]+reduced_4_2_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4_2)
evulation_summary


```

## now looking at outliers (with "best possible")

```{r}
summary(reduced_4)

```


# Now looking into outliers/influence

```{r}
p <- 12
n <- 5197
```

### Cooks
```{r}
reduced_4_cook <-cooks.distance(reduced_4)
reduced_4_cook[reduced_4_cook>qf(0.5,p,n-p)]
```
### DFFITs

```{r}
##dffits
DFFITS<-dffits(reduced_4)
DDFFITS_influence <- DFFITS[abs(DFFITS)>2*sqrt(p/n)]
DDFFITS_influence
```
### DFBETAs
```{r}
DFBETAS<-dfbetas(reduced_4)
abs(DFBETAS)>2/sqrt(n)
```


### leverage
```{r}
##leverages
lev<-lm.influence(reduced_4)$hat
##identify high leverage points
leverages <- lev[lev>2*p/n]
leverages
```


### outlier

```{r}
reduced_4.res <- reduced_4$residuals
crit<-qt(1-0.05/(2*n), n-p-1)
outliers <- reduced_4.res[abs(reduced_4.res)>crit]
outliers
```




```{r}
## outliers removed
outliers_index <- attr(outliers, "names")
outliers_index <- as.numeric(outliers_index)
train_no_outliers <- train[-(outliers_index),]

#leverages removed
lererages_index <- attr(leverages, "names")
lererages_index <- as.numeric(lererages_index)
train_no_leverages <- train[-(lererages_index),]

# DDFFITS_influence
DDFFITS_index <- attr(DDFFITS_influence, "names")
DDFFITS_index <- as.numeric(DDFFITS_index)
train_no_DDFFITS <- train[-(DDFFITS_index),]

# all "non-normal" removed
all_special <- c(DDFFITS_index,lererages_index,outliers_index)
train_nothing_special <- train[-(all_special),]
train_nothing_special




```
```{r}
vif(train[c(2,3,4,7,8,6,9,10,11)])
```

```{r}
train_temp<-train
# as.factor(train_temp$Type)<-numeric(train_temp$Type)
#train_temp
# as.factor

train_temp$Type <- as.numeric(train_temp$Type)-1
train_temp$Type <- as.integer(train_temp$Type)
train_temp
```



## creating reduced 

```{r}
reduced_4_3 <- glm(cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type, family=binomial, data=train_no_outliers)

reduced_4_4_lev <- glm(cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type, family=binomial, data=train_no_leverages)

reduced_4_5_DDFFITS <- glm(cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type, family=binomial, data=train_no_DDFFITS)

reduced_4_6_no_special <- glm(cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type, family=binomial, data=train_nothing_special)
# summary(reduced_4_6_no_special)



```


```{r}
## checking colinearity / VIF scores
# reduced_4_3_col <- data.frame('reduced_4_3' = check_collinearity(reduced_4_3))
# reduced_4_3_col_VIF <- reduced_4_3_col[c('reduced_4_3.Term','reduced_4_3.VIF')]
# reduced_4_3_col_VIF
# 
# reduced_4_4_lev_col <- data.frame('reduced_4_4_lev' = check_collinearity(reduced_4_4_lev))
# reduced_4_4_lev_col_VIF <- reduced_4_4_lev_col[c('reduced_4_4_lev.Term','reduced_4_4_lev.VIF')]
# reduced_4_4_lev_col_VIF
# 
# 
# reduced_4_5_DDFFITS_col <- data.frame('reduced_4_5_DDFFITS' = check_collinearity(reduced_4_5_DDFFITS))
# reduced_4_5_DDFFITS_col_VIF <- reduced_4_5_DDFFITS_col[c('reduced_4_5_DDFFITS.Term','reduced_4_5_DDFFITS.VIF')]
# reduced_4_5_DDFFITS_col_VIF
# 
# reduced_4_6_no_special_col <- data.frame('reduced_4_6_no_special' = check_collinearity(reduced_4_6_no_special))
# reduced_4_6_no_special_col_VIF <- reduced_4_6_no_special_col[c('reduced_4_6_no_special.Term','reduced_4_6_no_special.VIF')]
# reduced_4_6_no_special_col_VIF
# 
# VIF_summary <- data.frame('0'=reduced_4_3_col_VIF['reduced_4_3.Term'],
#                           '1'=reduced_4_3_col_VIF['reduced_4_3.VIF'],
#                           '2'=reduced_4_4_lev_col_VIF['reduced_4_4_lev.VIF'],
#                           '3'=reduced_4_5_DDFFITS_col_VIF['reduced_4_5_DDFFITS.VIF'],
#                           '4'=reduced_4_6_no_special_col_VIF['reduced_4_6_no_special.VIF'])
# colnames(VIF_summary) <- c('Predictor Variable','4_3.VIF.Outliers','4_4_lev.VIF','4_5_DDFFITS.VIF','4_6_no_special.VIF')
# VIF_summary

## VIF for Outliers
### cat_quality~volatile.acidity+citric.acid+residual.sugar+total.sulfur.dioxide+density+free.sulfur.dioxide+pH+sulphates+alcohol+Type


#
reg_4_VIF_test <- vif(train_temp[c(1,2,3,4,7,8,6,9,10,11,12)])
reg_4_2_VIF_test <- vif(train_temp[c(2,3,4,7,8,6,9,10,11,12)])
outliers_VIF <- vif(train_no_outliers[c(2,3,4,7,8,6,9,10,11,12)])
leverage_VIF <- vif(train_no_leverages[c(2,3,4,7,8,6,9,10,11,12)])
DDFFITS_VIF <- vif(train_no_DDFFITS[c(2,3,4,7,8,6,9,10,11,12)])
nothing_special <- vif(train_nothing_special[c(2,3,4,7,8,6,9,10,11,12)])


reg_4_VIF_test

VIF_summary_test <- data.frame('best_possible_VIF (post)'=reg_4_2_VIF_test,
                               'outliers_VIF'=outliers_VIF,
                               'leverage_VIF'=leverage_VIF,
                               'DDFFITS_VIF'= DDFFITS_VIF,
                               'nothing_special'=nothing_special)
VIF_summary_test

```


```{r}
##evaluating model
Reduced4_3_AIC_train <- reduced_4_3$aic

##predicted quality for test data based on training data
preds<-predict(reduced_4_3,newdata=test, type="response")

reduced_4_3_error <- table(test$cat_quality, preds>0.6)

evulation_summary_4_3 <- data.frame(
  attempt = 'reduced_4_3_error_outliers',
  AIC = Reduced4_3_AIC_train,
  PRESS = get_press(reduced_4_3),
  'False positive' = round(reduced_4_3_error[3]/(reduced_4_3_error[1]+reduced_4_3_error[3]),3),
  'False negative' = round(reduced_4_3_error[2]/(reduced_4_3_error[2]+reduced_4_3_error[4]),3),
  'Error Rate' = round((reduced_4_3_error[2]+reduced_4_3_error[3])/(reduced_4_3_error[1]+reduced_4_3_error[2]+reduced_4_3_error[3]+reduced_4_3_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4_3)
evulation_summary
```


```{r}
##evaluating model leverage
reduced_4_4_lev_AIC_train <- reduced_4_4_lev$aic

##predicted quality for test data based on training data
preds<-predict(reduced_4_4_lev,newdata=test, type="response")

reduced_4_4_lev_error <- table(test$cat_quality, preds>0.65)

evulation_summary_4_4_lev <- data.frame(
  attempt = 'reduced_4_4_lev_error',
  AIC = reduced_4_4_lev_AIC_train,
  PRESS = get_press(reduced_4_4_lev),
  'False positive' = round(reduced_4_4_lev_error[3]/(reduced_4_4_lev_error[1]+reduced_4_4_lev_error[3]),3),
  'False negative' = round(reduced_4_4_lev_error[2]/(reduced_4_4_lev_error[2]+reduced_4_4_lev_error[4]),3),
  'Error Rate' = round((reduced_4_4_lev_error[2]+reduced_4_4_lev_error[3])/(reduced_4_4_lev_error[1]+reduced_4_4_lev_error[2]+reduced_4_4_lev_error[3]+reduced_4_4_lev_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4_4_lev)
evulation_summary
```



```{r}
##evaluating model DDFFITS
reduced_4_5_DDFFITS_AIC_train <- reduced_4_5_DDFFITS$aic

##predicted quality for test data based on training data
preds<-predict(reduced_4_5_DDFFITS,newdata=test, type="response")

reduced_4_5_DDFFITS_error <- table(test$cat_quality, preds>0.7)

evulation_summary_4_5_DDFFITS <- data.frame(
  attempt = 'reduced_4_5_DDFFITS_error',
  AIC = reduced_4_5_DDFFITS_AIC_train,
  PRESS = get_press(reduced_4_5_DDFFITS),
  'False positive' = round(reduced_4_5_DDFFITS_error[3]/(reduced_4_5_DDFFITS_error[1]+reduced_4_5_DDFFITS_error[3]),3),
  'False negative' = round(reduced_4_5_DDFFITS_error[2]/(reduced_4_5_DDFFITS_error[2]+reduced_4_5_DDFFITS_error[4]),3),
  'Error Rate' = round((reduced_4_5_DDFFITS_error[2]+reduced_4_5_DDFFITS_error[3])/(reduced_4_5_DDFFITS_error[1]+reduced_4_5_DDFFITS_error[2]+reduced_4_5_DDFFITS_error[3]+reduced_4_5_DDFFITS_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4_5_DDFFITS)
evulation_summary
```

```{r}
##evaluating model DDFFITS
reduced_4_6_no_special_AIC_train <- reduced_4_6_no_special$aic

##predicted quality for test data based on training data
preds<-predict(reduced_4_6_no_special,newdata=test, type="response")

reduced_4_6_no_special_error <- table(test$cat_quality, preds>0.8)

evulation_summary_4_6_no_special <- data.frame(
  attempt = 'reduced_4_6_no_special_error',
  AIC = reduced_4_6_no_special_AIC_train,
  PRESS = get_press(reduced_4_6_no_special),
  'False positive' = round(reduced_4_6_no_special_error[3]/(reduced_4_6_no_special_error[1]+reduced_4_6_no_special_error[3]),3),
  'False negative' = round(reduced_4_6_no_special_error[2]/(reduced_4_6_no_special_error[2]+reduced_4_6_no_special_error[4]),3),
  'Error Rate' = round((reduced_4_6_no_special_error[2]+reduced_4_6_no_special_error[3])/(reduced_4_6_no_special_error[1]+reduced_4_6_no_special_error[2]+reduced_4_6_no_special_error[3]+reduced_4_6_no_special_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_4_6_no_special)
evulation_summary
```


## ROC Curves and AUC
```{r}
## reduced_1
# detach(package:performance, unload=TRUE)
## FYI the performance package causes ROC curves to not work
library(ROCR)



# reduced_1
preds<-predict(reduced_1,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_1")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_1_auc <- auc@y.values

## reduced_4
preds<-predict(reduced_4,newdata=test, type="response")
rates4<-prediction(preds, test$cat_quality)
roc_result<-performance(rates4,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4")
lines(x = c(0,1), y = c(0,1), col="red")

auc4<-performance(rates4, measure = "auc")
reduced_4_auc <- auc4@y.values

## reduced_4_2
preds<-predict(reduced_4_2,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_2")
lines(x = c(0,1), y = c(0,1), col="red")

auc4_2<-performance(rates, measure = "auc")
reduced_4_2_auc <- auc4_2@y.values

## reduced_4_3
preds<-predict(reduced_4_3,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_3")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_3_auc <- auc@y.values

## reduced_4_4_lev 
preds<-predict(reduced_4_4_lev,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_4_lev")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_4_lev_auc <- auc@y.values

## reduced_4_5_DDFFITS 
preds<-predict(reduced_4_5_DDFFITS,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_5_DDFFITS")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_5_DDFFITS_auc <- auc@y.values

## reduced_4_6_no_special 
preds<-predict(reduced_4_6_no_special,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_6_no_special")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_6_no_special_auc <- auc@y.values

AUC_summary <- data.frame('reduced_1'=reduced_1_auc,
                          'reduced_4'=reduced_4_auc,
                          'reduced_4_2'=reduced_4_2_auc,
                          'reduced_4_3'=reduced_4_3_auc,
                          'reduced_4_4_lev'=reduced_4_4_lev_auc,
                          'reduced_4_5_DDFFITS'=reduced_4_5_DDFFITS_auc,
                          'reduced_4_6_no_special'=reduced_4_6_no_special_auc)
colnames(AUC_summary) <- c('reduced_1','reduced_4','reduced_4_2','reduced_4_3','reduced_4_4_lev','reduced_4_5_DDFFITS','reduced_4_6_no_special')

AUC_summary
```





## Ryan's part starts here

### The goal is to make 3 models: One for just white, one for just red, and one with interaction terms with the type of wine. 

#### After that, the models will be trained on the filtered datasets and the resulting scores will be added to the evaluation summary.


## Red wine only model
```{r}
regfull_Red<-glm(cat_quality~., family="binomial", data=train_Red_NoType)
regnull_Red<-glm(cat_quality~1, family="binomial", data=train_Red_NoType)
step(regnull_Red, scope=list(lower=regnull_Red, upper=regfull_Red), direction="forward")

```


The model looks great after the foward selection! Time to test and add to the evaluation summary.
```{r}
model1_Red<-glm(formula = cat_quality ~ alcohol + volatile.acidity + total.sulfur.dioxide + 
    sulphates + free.sulfur.dioxide + chlorides, family = "binomial", 
    data = train_Red_NoType)

summary(model1_Red)


```

```{r}

##evaluating model
model1_Red_AIC_train <- model1_Red$aic
##predicted quality for test data based on training data
test_Red_NoType<-subset(test, Type == 0, select=-c(Type))
preds<-predict(model1_Red,newdata=test_Red_NoType, type="response")
model1_Red_error <- table(test_Red_NoType$cat_quality, preds>0.7)
#Curves
evulation_summary_1R <- data.frame(
  attempt = 'model1_Red',
  AIC = model1_Red_AIC_train,
  PRESS = get_press(model1_Red),
  'False positive' = round(model1_Red_error[3]/(model1_Red_error[1]+model1_Red_error[3]),3),
  'False negative' = round(model1_Red_error[2]/(model1_Red_error[2]+model1_Red_error[4]),3),
  'Error Rate' = round((model1_Red_error[2]+model1_Red_error[3])/(model1_Red_error[1]+model1_Red_error[2]+model1_Red_error[3]+model1_Red_error[4]),3)
)

compare_models<-rbind(evulation_summary[1,],evulation_summary_1R)
compare_models

evulation_summary <- rbind(evulation_summary,evulation_summary_1R)
evulation_summary

```
```{r}
# model1_Red
library(ROCR)
preds<-predict(model1_Red,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_Red")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_Red_auc <- auc@y.values

```

## White wine only model

```{r}
regfull_White<-glm(cat_quality~., family="binomial", data=train_White_NoType)
regnull_White<-glm(cat_quality~1,family="binomial", data=train_White_NoType)
step(regnull_White, scope=list(lower=regnull_White, upper=regfull_White), direction="forward")
```


The model looks good after the foward selection, but the predictor fixed.acidity can be removed. The density VIF is above ten, but jsut barely. For now, it will be left in. Time to test and add to the evaluation summary.
```{r}
model1_White<-glm(formula = cat_quality ~ alcohol + volatile.acidity + residual.sugar + 
    fixed.acidity + sulphates + free.sulfur.dioxide + density + 
    pH, family = "binomial", data = train_White_NoType)


summary(model1_White)


model1_White<-glm(formula = cat_quality ~ alcohol + volatile.acidity + residual.sugar + sulphates + free.sulfur.dioxide + density + 
    pH, family = "binomial", data = train_White_NoType)

summary(model1_White)


```


```{r}

##evaluating model
model1_White_AIC_train <- model1_White$aic
##predicted quality for test data based on training data
test_White_NoType<-subset(test, Type == 1, select=-c(Type))
preds<-predict(model1_White,newdata=test_White_NoType, type="response")
model1_White_error <- table(test_White_NoType$cat_quality, preds>0.7)
#Curves
evulation_summary_1W <- data.frame(
  attempt = 'model1_White',
  AIC = model1_White_AIC_train,
  PRESS = get_press(model1_White),
  'False positive' = round(model1_White_error[3]/(model1_White_error[1]+model1_White_error[3]),3),
  'False negative' = round(model1_White_error[2]/(model1_White_error[2]+model1_White_error[4]),3),
  'Error Rate' = round((model1_White_error[2]+model1_White_error[3])/(model1_White_error[1]+model1_White_error[2]+model1_White_error[3]+model1_White_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_1W)
evulation_summary


compare_models<-rbind(compare_models,evulation_summary_1W)
compare_models

```



## The model with interaction terms

```{r}
regfull_int<-glm(cat_quality~.*Type, family="binomial", data=train)
regnull_int<-glm(cat_quality~1,family="binomial", data=train)
step(regnull_int, scope=list(lower=regnull_int, upper=regfull_int), direction="forward")
```


The forward step process dropped + sulphates:Type, fixed.acidity, alcohol:Type, and citric.acid  By the hierarchical principle, the two non-interactive terms need to be added back because their have interaction terms are in the model.
```{r}
 model1_int<-glm(formula = cat_quality ~ alcohol + volatile.acidity + density + 
    sulphates + residual.sugar +  Type + total.sulfur.dioxide + 
    free.sulfur.dioxide + pH + chlorides + volatile.acidity:Type + 
    Type:total.sulfur.dioxide + density:Type + residual.sugar:Type + 
    Type:pH + Type:chlorides + Type:free.sulfur.dioxide, family = "binomial", 
    data = train)
summary(model1_int)

```


```{r}

##evaluating model
model1_int_AIC_train <- model1_int$aic
##predicted quality for test data based on training data
preds<-predict(model1_int,newdata=test, type="response")
model1_int_error <- table(test$cat_quality, preds>0.7)
#Curves
evulation_summary_1int <- data.frame(
  attempt = 'model1_int',
  AIC = model1_int_AIC_train,
  PRESS = get_press(model1_int),
  'False positive' = round(model1_int_error[3]/(model1_int_error[1]+model1_int_error[3]),3),
  'False negative' = round(model1_int_error[2]/(model1_int_error[2]+model1_int_error[4]),3),
  'Error Rate' = round((model1_int_error[2]+model1_int_error[3])/(model1_int_error[1]+model1_int_error[2]+model1_int_error[3]+model1_int_error[4]),3)
)

evulation_summary <- rbind(evulation_summary,evulation_summary_1int)
evulation_summary

compare_models<-rbind(compare_models,evulation_summary_1int)
compare_models

```

```{r}
compare_models<-compare_models%>% 
  rename(
    Model = attempt
    )

```


```{r}
library(data.table)
library(dplyr)
library(formattable)
library(tidyr)
customGreen0 = "#DeF7E9"

customGreen = "#71CA97"

customRed = "#ff7f7f"

```













Creating the ROC curves and AUC for the 3 new models.
```{r}
# model1_Red
preds<-predict(model1_Red,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_Red")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_Red_auc <- auc@y.values


# model1_White
preds<-predict(model1_White,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_White")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_White_auc <- auc@y.values


# model1_int
preds<-predict(model1_int,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_int")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_int_auc <- auc@y.values
```


```{r}
reduced_1_auc
model1_Red_auc
model1_White_auc
model1_int_auc
```





















## This is the one liners that run the tables and figures!





```{r}

##create heat map Consolidated
ggplot(data = melted_cor_train, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'Consolidated (Both Red and White)')

```

```{r}
##create heat map White
ggplot(data = melted_cor_train_white, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'White Wine')

```

```{r}
##create heat map Red
ggplot(data = melted_cor_train_Red, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  coord_fixed()+
  labs(title = 'Red Wine')

```





```{r}


ggplot(train_with_qual, mapping = aes(x=quality, fill=Type))+
  geom_histogram(binwidth=1, alpha=.4, position="identity", color="black")+
  geom_vline(aes(xintercept=5.5, color="red"),
             linetype="dashed")+
  scale_color_manual(name = "Cut Off", values = c("red"))+
  labs(x="Quality",
       y="Frequency",
       title="Distribution of Quality Rating by Wine Type")
```



This is the table for showing the evaluation for the first model
```{r}
formattable(evulation_summary[1,])
```



```{r}
summary(model1_Red)
```

```{r}
summary(model1_White)
```


```{r}
summary(model1_int)
```




Table right above the "Best Possible Model (Reduced_4)" section.
```{r}
formattable(compare_models,align =c("l","c", "c", "c", "c", "r"))

```




This is the table for showing the best models (top five)
```{r}
formattable(res.best.logistic$BestModels)
```


Add ROC for reduced_1, model1_Red, model1_White, model1_int

```{r}

preds<-predict(reduced_1,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_1")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_1_auc <- auc@y.values

```


```{r}
# model1_Red
preds<-predict(model1_Red,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_Red")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_Red_auc <- auc@y.values

```

```{r}

# model1_White
preds<-predict(model1_White,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_White")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_White_auc <- auc@y.values

```

```{r}

# model1_int
preds<-predict(model1_int,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for model1_int")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
model1_int_auc <- auc@y.values
```


This is the table for showing the best models (top five)
```{r}
formattable(evulation_summary[2,])
```


This is the table for reduced_4 VIF.
```{r}
formattable(data.frame(reg_4_VIF_test), align =c("l","r"))
```

This is the next VIF plot in the report
```{r}
formattable(data.frame(reg_4_2_VIF_test), align =c("l","r"))
```


The table below that. It is the evaluation summary for reduced_4_2
```{r}
formattable(evulation_summary[3,])
```


evaluation summary for the outlier/leverage/etc.
```{r}
formattable(evulation_summary[4:7,])
```



add roc curves for these four.

```{r}


## reduced_4
preds<-predict(reduced_4,newdata=test, type="response")
rates4<-prediction(preds, test$cat_quality)
roc_result<-performance(rates4,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4")
lines(x = c(0,1), y = c(0,1), col="red")

auc4<-performance(rates4, measure = "auc")
reduced_4_auc <- auc4@y.values

```

```{r}

## reduced_4_2
preds<-predict(reduced_4_2,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_2")
lines(x = c(0,1), y = c(0,1), col="red")

auc4_2<-performance(rates, measure = "auc")
reduced_4_2_auc <- auc4_2@y.values

```


```{r}

## reduced_4_3
preds<-predict(reduced_4_3,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_3")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_3_auc <- auc@y.values


```


```{r}
## reduced_4_4_lev 
preds<-predict(reduced_4_4_lev,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_4_lev")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_4_lev_auc <- auc@y.values

```

```{r}

## reduced_4_5_DDFFITS 
preds<-predict(reduced_4_5_DDFFITS,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_5_DDFFITS")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_5_DDFFITS_auc <- auc@y.values

```




```{r}

## reduced_4_6_no_special 
preds<-predict(reduced_4_6_no_special,newdata=test, type="response")
rates<-prediction(preds, test$cat_quality)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for reduced_4_6_no_special")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
reduced_4_6_no_special_auc <- auc@y.values

```

```{r}
AUC_summary <- data.frame('reduced_1'=reduced_1_auc,
                          'reduced_4'=reduced_4_auc,
                          'reduced_4_2'=reduced_4_2_auc,
                          'reduced_4_3'=reduced_4_3_auc,
                          'reduced_4_4_lev'=reduced_4_4_lev_auc,
                          'reduced_4_5_DDFFITS'=reduced_4_5_DDFFITS_auc,
                          'reduced_4_6_no_special'=reduced_4_6_no_special_auc)
colnames(AUC_summary) <- c('reduced_1','reduced_4','reduced_4_2','reduced_4_3','reduced_4_4_lev','reduced_4_5_DDFFITS','reduced_4_6_no_special')

AUC_summary
```


```{r}
formattable(evulation_summary[1:7,])
```












## First model and additional EDA

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(ROCR)
library(ggplot2)
library(MASS)
library(faraway)
```


```{r include=FALSE}
## Load Datasets
full_wines_final <- read.csv("Data_Final.csv", header = TRUE, stringsAsFactors=TRUE)
# Drop quality for simplicity
full_wines_binary <- subset(full_wines_final, select = -c(cat_quality))

set.seed(90210) ##for reproducibility
sample<-sample.int(nrow(full_wines_binary), floor(.80*nrow(full_wines_binary)), replace = F)
train<-full_wines_binary[sample, ] ##training data frame
test<-full_wines_binary[-sample, ] ##test data frame
```

The first research question focused on exploring the relationships between different chemical features in the dataset and wine type (categorized as either red or white). Since the response variable, wine type, was a Bernoulli random variable and the observations were independent, a logistic regression was used for the predictive model.

As seen above in the previous section, exploratory data analysis was performed in order to better understand the relationships between individual predictor variables and wine type. Density plots visualized the quantitative predictors, while frequency bar charts visualized the categorical wine quality predictor. The majority of these features were distributed differently across wine type, indicating that they could be used to differentiate between red and white wine in the model. However, as seen in Figure XXX, alcohol, pH, and sulphates do seem to have a weaker association with wine type since their red and white density plots look somewhat similar.
```{r echo=FALSE}
ggplot(train, aes(x=Type))+
  geom_bar()+
  labs(x="Wine Type", y="count",
       title="Wine Type")

ggplot(train, aes(x=quality, fill=Type))+
  geom_bar(position = "fill")+
  labs(x="Wine Quality", y="Proportion",
       title="Wine Quality by Type")

##density plots
ggplot(train,aes(x=volatile.acidity, color=Type))+
  geom_density()+
  labs(title="Density Plot of Volatile Acidity by Type")

ggplot(train,aes(x=citric.acid, color=Type))+
  geom_density()+
  labs(title="Density Plot of Citric Acid by Type")

ggplot(train,aes(x=residual.sugar, color=Type))+
  geom_density()+
  labs(title="Density Plot of Residual Sugars by Type")

ggplot(train,aes(x=chlorides, color=Type))+
  geom_density()+
  labs(title="Density Plot of Chlorides Sugars by Type")

ggplot(train,aes(x=free.sulfur.dioxide, color=Type))+
  geom_density()+
  labs(title="Density Plot of Free Sulfur Dioxide by Type")

ggplot(train,aes(x=total.sulfur.dioxide, color=Type))+
  geom_density()+
  labs(title="Density Plot of Total Sulfur Dioxide by Type")

ggplot(train,aes(x=density, color=Type))+
  geom_density()+
  labs(title="Density Plot of Density by Type")

ggplot(train,aes(x=pH, color=Type))+
  geom_density()+
  labs(title="Density Plot of pH by Type")

ggplot(train,aes(x=sulphates, color=Type))+
  geom_density()+
  labs(title="Density Plot of Sulphates by Type")

ggplot(train,aes(x=alcohol, color=Type))+
  geom_density()+
  labs(title="Density Plot of Alcohol by Type")
```

After exploring these features, an initial model was generated with all of the predictors using the random training sample from the combined wine dataset. From the model summary, the chemical properties of fixed acidity, citric acid, pH, and sulphates all appeared to have insignificant z-scores. To determine if all of these features could be dropped from the model, a likelihood ratio test comparing the full and reduced model was conducted. This test resulted in an insignificant p-value of 0.1360. Therefore, there was not a significant difference between the two models and the reduced model could be adopted. This model’s performance is summarized below. XXX
```{r echo=FALSE}
log_mod<- glm(Type ~., family='binomial', data=train)
summary(log_mod)
```

```{r echo=FALSE}
preds<-predict(log_mod,newdata=test, type="response")

rates<-prediction(preds, test$Type)

roc_result<-performance(rates,measure="tpr", x.measure="fpr")

plot(roc_result, main="ROC Curve for Full Wine Type Model")
lines(x = c(0,1), y = c(0,1), col="red")
```

```{r echo=FALSE}
auc<-performance(rates, measure = "auc")
auc@y.values
```

```{r echo=FALSE}
table(test$Type, preds>0.65)
```

```{r echo=FALSE}
reduced_mod<- glm(Type ~. -quality-pH-fixed.acidity-citric.acid, family='binomial', data=train)
summary(reduced_mod)
```

```{r include=FALSE}
g <- reduced_mod$deviance-log_mod$deviance
1-pchisq(g,4)
```

The ROC curve XXX, seen below, indicates that the model performed well (significantly better than random guessing shown by the red line). Furthermore, the AUC, which represents the area under this curve, was 0.994. These performance metrics suggest that the model is quite effective at predicting the wine type of the training data. When this model predicted the wine type of the testing data (the sample leftover from the training set), it maintained this high performance. We used a probability floor of 0.65 since this value minimized overall error (we saw no practical difference between type I and type II errors in this context). Using this value, the model had an overall error rate of only 0.0062. This result demonstrates the effectiveness of the model when presented with data it had not encountered in training. 
```{r echo=FALSE}
preds<-predict(reduced_mod,newdata=test, type="response")

rates<-prediction(preds, test$Type)

roc_result<-performance(rates,measure="tpr", x.measure="fpr")

plot(roc_result, main="ROC Curve for Reduced Wine Type Model")
lines(x = c(0,1), y = c(0,1), col="red")
```

```{r include=FALSE}
auc<-performance(rates, measure = "auc")
auc@y.values
```


```{r echo=FALSE}
table(test$Type, preds>0.65)
```

To further improve the model’s performance, the presence of outliers in the training data was considered. Using Cook’s distance, one outlier (with a distance of 3.0858) was identified. However, after refitting a model without this outlier, there was a very small difference in overall performance as seen below XXX. Thus, we elected to keep the outlier in the data—especially, since we had little insight into the data and could not deduce whether it was a corrupted datapoint.
```{r echo=FALSE}
n<-dim(train)[1]
p<-9

##cooks distance
outlier<- train
outlier['COOKS']<- cooks.distance(reduced_mod)
outlier[outlier$COOKS>qf(0.5,p,n-p),]
```

```{r include=FALSE}
new_data<-outlier[!outlier$COOKS>qf(0.5,p,n-p),-14]
new_full_mod<- glm(Type ~., family='binomial', data=new_data)
summary(new_full_mod)
```
```{r echo=FALSE}
new_reduced_mod<- glm(Type ~. -quality-sulphates-citric.acid, family='binomial', data=new_data)
summary(new_reduced_mod)
```

```{r echo=FALSE}
preds<-predict(new_reduced_mod,newdata=test, type="response")

rates<-prediction(preds, test$Type)

roc_result<-performance(rates,measure="tpr", x.measure="fpr")

plot(roc_result, main="ROC Curve for Wine Type")
lines(x = c(0,1), y = c(0,1), col="red")
```

```{r include=FALSE}
auc<-performance(rates, measure = "auc")
auc@y.values
```

```{r echo=FALSE}
table(test$Type, preds>0.4)
```

Finally, collinearity among the predictors was examined in the reduced model. Using the VIF formula, density was very correlated with the other predictors (having a VIF value of 13.714). Since this value was over 10, this parameter was dropped in order to reduce collinearity and by extension the predicted coefficients’ variances. In addition, the correlation matrix of the predictors showed a strong association between free sulfur dioxide and total sulfur dioxide (correlation of 0.7156). Since one of these features could explain much of the variation in the other, it was determined one should be dropped. When comparing models with only one of these features, it was evident that total sulfur dioxide was more important in predicting wine type than free sulfur dioxide. Therefore, free sulfur dioxide was dropped from the model. The performance of this new reduced model can be seen below XXX.
```{r echo=FALSE}
x<- train[,c(1,2,4,5,6,7,8,9,11)]
vif(x)
```
```{r echo=FALSE}
cor(x)
```

```{r echo=FALSE}
reduced_mod_final<- glm(Type ~. -quality-pH-fixed.acidity-citric.acid-density-free.sulfur.dioxide, family='binomial', data=train)
summary(reduced_mod_final)
```

```{r echo=FALSE}
preds<-predict(reduced_mod_final,newdata=test, type="response")

rates<-prediction(preds, test$Type)

roc_result<-performance(rates,measure="tpr", x.measure="fpr")

plot(roc_result, main="ROC Curve for Final Wine Type Model")
lines(x = c(0,1), y = c(0,1), col="red")
```

```{r include=FALSE}
auc<-performance(rates, measure = "auc")
auc@y.values
```

```{r echo=FALSE}
table(test$Type, preds>0.8)
```

Though the model’s performance decreased slightly (overall error rate went from 0.0062 to 0.0297 and AUC went from 0.9947 to 0.9892), the decrease in variance of the coefficients should make the model more reliable in terms of interpreting the relationships between the predictors and wine type.

Overall, this final model is quite effective at determining wine type. When looking at the estimated coefficients, it seems as though higher residual sugar, total sulfur dioxide, and alcohol are associated with white wines, while higher volatile acidity, chlorides, and sulphates are associated with red wines. Together, these predictors explain much of the variation in the predictor variable, wine type (residual deviance of only 286.49). 


